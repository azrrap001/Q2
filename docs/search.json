[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STATISTICAL COMPUTING PRACTICALS",
    "section": "",
    "text": "Here’s a quick overview of what you’ll find:\n\nPractical one : (PRAC_1.qmd)\n\nPractical Day Four which includes the lm() function done from first principles at the end: (prac-day-four.qmd)\n\nPractical two : (prac_2.qmd)\n\nHere is the link to my github repository : github repository.\nLocate to the qmd file to access all the .qmd and .yml documents for this assignment.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>STATISTICAL COMPUTING PRACTICALS</span>"
    ]
  },
  {
    "objectID": "PRAC_1.html",
    "href": "PRAC_1.html",
    "title": "Practical_One Solutions",
    "section": "",
    "text": "Question 1: Missing Values\n1st 6 rows of airquality dataset displayed below:\n\nhead(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\n\nDisplay rows with missing values:\n\n# Find rows with missing values  \nmissing_values &lt;- airquality[!complete.cases(airquality), ]  \nmissing_values\n\n    Ozone Solar.R Wind Temp Month Day\n5      NA      NA 14.3   56     5   5\n6      28      NA 14.9   66     5   6\n10     NA     194  8.6   69     5  10\n11      7      NA  6.9   74     5  11\n25     NA      66 16.6   57     5  25\n26     NA     266 14.9   58     5  26\n27     NA      NA  8.0   57     5  27\n32     NA     286  8.6   78     6   1\n33     NA     287  9.7   74     6   2\n34     NA     242 16.1   67     6   3\n35     NA     186  9.2   84     6   4\n36     NA     220  8.6   85     6   5\n37     NA     264 14.3   79     6   6\n39     NA     273  6.9   87     6   8\n42     NA     259 10.9   93     6  11\n43     NA     250  9.2   92     6  12\n45     NA     332 13.8   80     6  14\n46     NA     322 11.5   79     6  15\n52     NA     150  6.3   77     6  21\n53     NA      59  1.7   76     6  22\n54     NA      91  4.6   76     6  23\n55     NA     250  6.3   76     6  24\n56     NA     135  8.0   75     6  25\n57     NA     127  8.0   78     6  26\n58     NA      47 10.3   73     6  27\n59     NA      98 11.5   80     6  28\n60     NA      31 14.9   77     6  29\n61     NA     138  8.0   83     6  30\n65     NA     101 10.9   84     7   4\n72     NA     139  8.6   82     7  11\n75     NA     291 14.9   91     7  14\n83     NA     258  9.7   81     7  22\n84     NA     295 11.5   82     7  23\n96     78      NA  6.9   86     8   4\n97     35      NA  7.4   85     8   5\n98     66      NA  4.6   87     8   6\n102    NA     222  8.6   92     8  10\n103    NA     137 11.5   86     8  11\n107    NA      64 11.5   79     8  15\n115    NA     255 12.6   75     8  23\n119    NA     153  5.7   88     8  27\n150    NA     145 13.2   77     9  27\n\n\n\n\nQuestion 2: Find mean, sd,min,max for Temperature and Ozone Level\nOutput and Code for mean, sd, min, max for each of temperature and ozone level, accounting for missing values\nTemperature and Ozone Summary Statistics:\n\nmean_temp &lt;- round(mean(airquality$Temp, na.rm=T),2)\nsd_temp &lt;- round(sd(airquality$Temp, na.rm=T),2)\nmin_temp &lt;- min(airquality$Temp, na.rm=T)\nmax_temp &lt;- max(airquality$Temp, na.rm=T)\n\nmean_ozone &lt;- round(mean(airquality$Ozone, na.rm=T),2)\nsd_ozone &lt;- round(sd(airquality$Ozone, na.rm=T),2)\nmin_ozone &lt;- min(airquality$Ozone, na.rm=T)\nmax_ozone &lt;- max(airquality$Ozone,na.rm=T)\n\nOutput of summary statistics for Temperature and Ozone layer:\n\n\n[1] \"Temperature - Mean: 77.88\"\n\n\n[1] \"Temperature - SD: 9.47\"\n\n\n[1] \"Temperature - Min: 56\"\n\n\n[1] \"Temperature - Max: 97\"\n\n\n[1] \"Ozone - Mean: 42.13\"\n\n\n[1] \"Ozone - SD: 32.99\"\n\n\n[1] \"Ozone - Min: 1\"\n\n\n[1] \"Ozone - Max: 168\"\n\n\n\n\nQuestion 3: Linear Regression for cars data\ncode displaying calculation of beta_hat estimates\n\n# Create the design matrix X (adding a column of ones for the intercept)  \nX &lt;- cbind(1, cars$speed)\n\n# Create the response variable Y  \nY &lt;- cars$dist  \n\n# Calculate the beta estimates using the  beta_hat formula \nbeta_hat &lt;- solve(t(X) %*% X) %*% (t(X) %*% Y)\nbeta_hat\n\n           [,1]\n[1,] -17.579095\n[2,]   3.932409\n\n\n\n\nThe beta estimates for B0 and B1 respectively are: -17.57909 3.932409 \n\n\nThe echo: false option disables the printing of code (only output is displayed).\n\n\nQuestion 4: Linear Regression for cars data\nCheck that the beta coefficients we obtained from the b_hat matrix is the same when fitting the linear regression model using lm() in R.\n\n model &lt;- lm(cars$dist~ cars$speed)\n summary(model)\n\n\nCall:\nlm(formula = cars$dist ~ cars$speed)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.069  -9.525  -2.272   9.215  43.201 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -17.5791     6.7584  -2.601   0.0123 *  \ncars$speed    3.9324     0.4155   9.464 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.38 on 48 degrees of freedom\nMultiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 \nF-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\n\n\n\nFunction to calculate statistics\nThis function is used to calculate the beta_hat estimates, p-value, standard error, and t-values of a simple linear regression model.\n\nf &lt;- function(x, y){\n  #set up X design matrix with column of ones for intercept \n  x &lt;-  cbind(1, x)\n  \n  #estimate regression coeficient i.e. b_hat \n  b_hat &lt;-  solve(t(x)%*%x)%*%t(x)%*%y\n  \n  #standard error of regression coeficients \n  Cmat &lt;- solve(t(x) %*% x)\n  #Estimate the residual VARIANCE, where sigma^2 is estimated by s^2\n  \n  #find k\n  k &lt;- ncol(x)\n  nobs &lt;- nrow(x)\n  # Find s2 Calculate the residuals sums of squares given by (Y-XB)^T(Y-XB)\n  rss &lt;- t(y - x %*% b_hat) %*% (y - x %*% b_hat)\n  #calculate s^2 = RSS / (n-k)\n  s2 &lt;-  as.numeric(rss)/(nobs - k) #since RSS is a 1x1 matrix convert to numeric scalar\n  \n  #find diagonals of C_mat\n  c_ii &lt;-  diag(Cmat)\n  # Calculate the standard error of the regression coefficients\n  std.error &lt;-  sqrt(s2 * c_ii)\n  \n  #Calculate the t-value \n  tval &lt;-  b_hat/std.error\n  \n  #Calculate p-value in Hypothesis testing \npval &lt;- 2 * (1 - pt(abs(tval), df = nobs - k))\n  \n  return(list('coeficients' = b_hat,\n              'standard error' = std.error,\n              't value' = tval,\n              'p-value' = pval\n    \n  ))\n}\n\nThis is an example of the above function using the cars dataset:\n\n(f(cars$speed, cars$dist))  \n\n$coeficients\n        [,1]\n  -17.579095\nx   3.932409\n\n$`standard error`\n                  x \n6.7584402 0.4155128 \n\n$`t value`\n       [,1]\n  -2.601058\nx  9.463990\n\n$`p-value`\n          [,1]\n  1.231882e-02\nx 1.489919e-12",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Practical_One Solutions</span>"
    ]
  },
  {
    "objectID": "prac-day-four.html",
    "href": "prac-day-four.html",
    "title": "Prac Day 4",
    "section": "",
    "text": "SET-UP:\n\n# Install tidyverse if not already installed\nif (!requireNamespace(\"tidyverse\", quietly = TRUE)) {\n  install.packages(\"tidyverse\")\n}\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Install and load nycflights13 for flight data\nif (!requireNamespace(\"nycflights13\", quietly = TRUE)) {\n  install.packages(\"nycflights13\")\n}\nif (!requireNamespace(\"remotes\", quietly = TRUE)) {\n  install.packages(\"remotes\")\n}\nremotes::install_github(\"SATVILab/UtilsDataRSV\")\n\nUsing GitHub PAT from the git credential store.\nSkipping install of 'UtilsDataRSV' from a github remote, the SHA1 (c7018542) has not changed since last install.\n  Use `force = TRUE` to force installation\n\nlibrary(nycflights13)\nlibrary(dplyr)  \nlibrary(tibble)\n\n\nQuestions\n\nDisplay the flights dataset in an alternative format to simply printing it (i.e. running flights).\n\n\n# Convert flights dataframe to a tibble\nUtilsDataRSV::view_cols(flights)\n\n[1] \"year\"\n[1] 1244\n[1] \"_____________________\"\n[1] \"month\"\n[1]  8 11  2  6  4\n[1] \"_____________________\"\n[1] \"day\"\n[1]  8 12 21  1 11\n[1] \"_____________________\"\n[1] \"dep_time\"\n[1]  159 1451 1722 2341   NA\n[1] \"_____________________\"\n[1] \"sched_dep_time\"\n[1] 2156 1620  647 1539 1323\n[1] \"_____________________\"\n[1] \"dep_delay\"\n[1] 406 135 287 212  NA\n[1] \"_____________________\"\n[1] \"arr_time\"\n[1]  150 1442  346  245   NA\n[1] \"_____________________\"\n[1] \"sched_arr_time\"\n[1]  847  825   29 2215  904\n[1] \"_____________________\"\n[1] \"arr_delay\"\n[1] -69 332 369 430  NA\n[1] \"_____________________\"\n[1] \"carrier\"\n [1] \"F9\" \"AA\" \"HA\" \"VX\" \"B6\" \"OO\" \"US\" \"9E\" \"WN\" \"YV\" \"MQ\" \"FL\" \"AS\" \"DL\" \"UA\"\n[16] \"EV\"\n[1] \"_____________________\"\n[1] \"flight\"\n[1]  721 4306 4470  652 1053\n[1] \"_____________________\"\n[1] \"tailnum\"\n [1] \"N3FVAA\" \"N417UA\" \"N983AT\" \"N5DMAA\" \"N14998\" \"N836AS\" \"N962DL\" \"N630JB\"\n [9] \"N24224\" \"N929AT\" \"N530UA\" \"N391SW\" \"N822MQ\" \"N697MQ\" \"N413WN\" \"N483AA\"\n[17] \"N806MQ\" \"N829MH\" \"N12126\" NA      \n[1] \"4024 unique entries not displayed\"\n[1] \"_____________________\"\n[1] \"origin\"\n[1] \"JFK\" \"EWR\" \"LGA\"\n[1] \"_____________________\"\n[1] \"dest\"\n [1] \"BWI\" \"BQN\" \"CAK\" \"TUL\" \"HNL\" \"SJU\" \"MSY\" \"CHO\" \"JAX\" \"PVD\" \"BZN\" \"PIT\"\n[13] \"CHS\" \"CLE\" \"MVY\" \"STT\" \"SEA\" \"MYR\" \"MCO\" \"GRR\"\n[1] \"85 unique entries not displayed\"\n[1] \"_____________________\"\n[1] \"air_time\"\n[1] 438 111 601 113  NA\n[1] \"_____________________\"\n[1] \"distance\"\n[1]  212 1023  892 1874 1795\n[1] \"_____________________\"\n[1] \"hour\"\n[1]  7  5 23  6 16\n[1] \"_____________________\"\n[1] \"minute\"\n[1] 34  2 26 43 48\n[1] \"_____________________\"\n[1] \"time_hour\"\n [1] \"2013-01-09 08:00:00 EST\" \"2013-09-17 07:00:00 EDT\"\n [3] \"2013-10-24 23:00:00 EDT\" \"2013-12-06 20:00:00 EST\"\n [5] \"2013-12-19 16:00:00 EST\" \"2013-02-12 18:00:00 EST\"\n [7] \"2013-09-03 13:00:00 EDT\" \"2013-11-18 14:00:00 EST\"\n [9] \"2013-06-26 12:00:00 EDT\" \"2013-10-10 22:00:00 EDT\"\n[11] \"2013-06-23 22:00:00 EDT\" \"2013-12-04 21:00:00 EST\"\n[13] \"2013-04-20 07:00:00 EDT\" \"2013-08-29 10:00:00 EDT\"\n[15] \"2013-06-05 21:00:00 EDT\" \"2013-02-06 18:00:00 EST\"\n[17] \"2013-05-21 08:00:00 EDT\" \"2013-02-02 07:00:00 EST\"\n[19] \"2013-10-12 09:00:00 EDT\" \"2013-02-10 19:00:00 EST\"\n[1] \"6916 unique entries not displayed\"\n[1] \"_____________________\"\n\n\nWarning: Not all unique entries displayed for these non-numeric cols: tailnum,\ndest, time_hour\n\nflights &lt;- as_tibble(flights)\nflights\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nNow inspect unique values in each column to flights tibble\n\nUtilsDataRSV::view_cols(flights)\n\n[1] \"year\"\n[1] 411\n[1] \"_____________________\"\n[1] \"month\"\n[1]  8  1  9  6 10\n[1] \"_____________________\"\n[1] \"day\"\n[1]  6 22 17 24  5\n[1] \"_____________________\"\n[1] \"dep_time\"\n[1] 1651 1737 1520 2207   NA\n[1] \"_____________________\"\n[1] \"sched_dep_time\"\n[1]  914 1410 1526 1810 1722\n[1] \"_____________________\"\n[1] \"dep_delay\"\n[1] 134 247 341 338  NA\n[1] \"_____________________\"\n[1] \"arr_time\"\n[1] 1404  547 1344 1325   NA\n[1] \"_____________________\"\n[1] \"sched_arr_time\"\n[1] 1740 1038 1554 1523  852\n[1] \"_____________________\"\n[1] \"arr_delay\"\n[1] 176 -38 856  72  NA\n[1] \"_____________________\"\n[1] \"carrier\"\n [1] \"VX\" \"FL\" \"AA\" \"US\" \"F9\" \"UA\" \"MQ\" \"B6\" \"AS\" \"HA\" \"DL\" \"OO\" \"YV\" \"WN\" \"EV\"\n[16] \"9E\"\n[1] \"_____________________\"\n[1] \"flight\"\n[1]  264 1816 3787 4510 1013\n[1] \"_____________________\"\n[1] \"tailnum\"\n [1] \"N13110\" \"N779SW\" \"N3ADAA\" \"N39416\" \"N371DA\" \"N137EV\" \"N5CFAA\" \"N718EV\"\n [9] \"N364NW\" \"N8696C\" \"N803SK\" \"N521LR\" \"N395SW\" \"N444US\" \"N773NC\" \"N349SW\"\n[17] \"N37465\" \"N685SW\" \"N8968E\" NA      \n[1] \"4024 unique entries not displayed\"\n[1] \"_____________________\"\n[1] \"origin\"\n[1] \"LGA\" \"EWR\" \"JFK\"\n[1] \"_____________________\"\n[1] \"dest\"\n [1] \"PSP\" \"ORD\" \"GSO\" \"MDW\" \"DEN\" \"BGR\" \"MVY\" \"PBI\" \"BZN\" \"BQN\" \"MEM\" \"DTW\"\n[13] \"BUR\" \"TVC\" \"ABQ\" \"SJC\" \"CHO\" \"SEA\" \"PIT\" \"BOS\"\n[1] \"85 unique entries not displayed\"\n[1] \"_____________________\"\n[1] \"air_time\"\n[1] 409 330  57 181  NA\n[1] \"_____________________\"\n[1] \"distance\"\n[1] 1428 1107   80 4983 1585\n[1] \"_____________________\"\n[1] \"hour\"\n[1] 16  8 13  9 11\n[1] \"_____________________\"\n[1] \"minute\"\n[1] 26  0 15 54 39\n[1] \"_____________________\"\n[1] \"time_hour\"\n [1] \"2013-02-17 13:00:00 EST\" \"2013-05-16 21:00:00 EDT\"\n [3] \"2013-07-17 14:00:00 EDT\" \"2013-08-04 10:00:00 EDT\"\n [5] \"2013-06-14 23:00:00 EDT\" \"2013-11-28 10:00:00 EST\"\n [7] \"2013-09-15 12:00:00 EDT\" \"2013-12-05 13:00:00 EST\"\n [9] \"2013-12-31 16:00:00 EST\" \"2013-08-29 19:00:00 EDT\"\n[11] \"2013-03-31 18:00:00 EDT\" \"2013-03-03 22:00:00 EST\"\n[13] \"2013-05-02 21:00:00 EDT\" \"2013-09-14 07:00:00 EDT\"\n[15] \"2013-12-16 10:00:00 EST\" \"2013-09-19 21:00:00 EDT\"\n[17] \"2013-04-21 15:00:00 EDT\" \"2013-09-03 23:00:00 EDT\"\n[19] \"2013-02-19 12:00:00 EST\" \"2013-11-28 09:00:00 EST\"\n[1] \"6916 unique entries not displayed\"\n[1] \"_____________________\"\n\n\nWarning: Not all unique entries displayed for these non-numeric cols: tailnum,\ndest, time_hour\n\n\n\nRewrite the following code using dplyr and the pipe:\nThis is the code before transformation.\n\nflight1 &lt;- flights[flights$month == 1, ]\n(carrier_vec &lt;- unique(flight1$carrier))\n\n [1] \"UA\" \"AA\" \"B6\" \"DL\" \"EV\" \"MQ\" \"US\" \"WN\" \"VX\" \"FL\" \"AS\" \"9E\" \"F9\" \"HA\" \"YV\"\n[16] \"OO\"\n\ncarrier_dist_vec_mean &lt;- numeric(length(carrier_vec))\ncarrier_dist_vec_sd &lt;- numeric(length(carrier_vec))\nfor (i in seq_along(carrier_vec)) {\n  carrier_dist_vec_mean[i] &lt;- mean(\n    flight1$distance[flight1$carrier == carrier_vec[i]]\n   )\n  carrier_dist_vec_sd[i] &lt;- sd(\n    flight1$distance[flight1$carrier == carrier_vec[i]]\n  )\n}\ndist_tbl &lt;- tibble(\n  carrier = carrier_vec,\n  mean_distance = carrier_dist_vec_mean,\n  sd_distance = carrier_dist_vec_sd\n)\ndist_tbl[order(dist_tbl$mean_distance), ]\n\n# A tibble: 16 × 3\n   carrier mean_distance sd_distance\n   &lt;chr&gt;           &lt;dbl&gt;       &lt;dbl&gt;\n 1 YV               229          0  \n 2 9E               476.       334. \n 3 EV               522.       294. \n 4 US               536.       553. \n 5 MQ               566.       223. \n 6 FL               691.       142. \n 7 OO               733         NA  \n 8 WN               942.       496. \n 9 B6              1062.       681. \n10 DL              1220.       644. \n11 AA              1350.       626. \n12 UA              1462.       778. \n13 F9              1620          0  \n14 AS              2402          0  \n15 VX              2495.        98.2\n16 HA              4983          0  \n\n\nThis is the code after transformation using dplyr and the pipe:\n\nflights |&gt; \n  filter(month == \"1\") |&gt;  #Filter the data to include only flights from January\n  group_by(carrier) |&gt; #Group the data  to perform calc. for each carrier.\n  summarise(\n    mean_distance = mean(distance, na.rm=T),#ignore NA values \n    sd_distance = sd(distance), na.rm=T) |&gt; \n  arrange(mean_distance)#ascending order \n\n# A tibble: 16 × 4\n   carrier mean_distance sd_distance na.rm\n   &lt;chr&gt;           &lt;dbl&gt;       &lt;dbl&gt; &lt;lgl&gt;\n 1 YV               229          0   TRUE \n 2 9E               476.       334.  TRUE \n 3 EV               522.       294.  TRUE \n 4 US               536.       553.  TRUE \n 5 MQ               566.       223.  TRUE \n 6 FL               691.       142.  TRUE \n 7 OO               733         NA   TRUE \n 8 WN               942.       496.  TRUE \n 9 B6              1062.       681.  TRUE \n10 DL              1220.       644.  TRUE \n11 AA              1350.       626.  TRUE \n12 UA              1462.       778.  TRUE \n13 F9              1620          0   TRUE \n14 AS              2402          0   TRUE \n15 VX              2495.        98.2 TRUE \n16 HA              4983          0   TRUE \n\n\nExplain why the standard deviation is NA for one carrier, and why it is 0 for others. Demonstrate your answer using code.\nStandard Deviation of NA occurs when observed flights has missing data in the distance column, and so when we calculate the standard deviation with those values we get an NA result.\nStandard Deviation of 0 indicates that all flights for a carrier have identical distances (no variability), leading to a standard deviation of 0. This could mean that carrier’s flights are operating under uniform conditions where they are scheduled to take the same route.\n\nbelow is the code demonstration:\n\nflights |&gt; \n  filter(month == \"1\") |&gt;  #Filter the data to include only flights from January\n  group_by(carrier) |&gt; #Group the data  to perform calc. for each carrier.\n  summarise(\n    sd_distance = sd(distance, na.rm=T),\n  ) |&gt; \n  #logical checks if either is TRUE row kept \nfilter(is.na(sd_distance)| sd_distance == 0)\n\n# A tibble: 5 × 2\n  carrier sd_distance\n  &lt;chr&gt;         &lt;dbl&gt;\n1 AS                0\n2 F9                0\n3 HA                0\n4 OO               NA\n5 YV                0\n\n\nThe code filters January flights to find carriers with either NA or 0 standard deviations, displaying relevant results.\n\nUsing tidyr and dplyr where appropriate, construct a dataframe where the carriers are along the columns, and the rows are the average departure delay (dep_delay) flown by each carrier (carrier) in each month.\n\nflights |&gt; \n  group_by(carrier,month) |&gt; #groups data by month and carrier \n  summarise(\n    avg_dep_delay = mean(dep_delay, na.rm = TRUE)) |&gt; \n  pivot_wider(\n    names_from = carrier,\n    values_from = avg_dep_delay\n  )\n\n`summarise()` has grouped output by 'carrier'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 12 × 17\n   month  `9E`    AA     AS    B6    DL    EV    F9    FL    HA    MQ    OO\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1 16.9   6.93  7.35   9.49  3.85 24.2  10     1.97 54.4   6.49 67   \n 2     2 16.5   8.28  0.722 13.8   5.54 21.5  29.8   5.18 17.4   8.09 NA   \n 3     3 13.4   8.70  8.42  14.2   9.93 26.2  16.8  17.3   1.16  7.19 NA   \n 4     4 13.6  11.7  11.3   15.2   8.17 22.8  24.6  13.1  -2.1  13.7  NA   \n 5     5 22.7   9.66  6.77   9.78  9.74 20.2  35.9  19.2  -1.45 13.9  NA   \n 6     6 29.0  14.6  13.1   20.4  18.7  25.5  29.4  38.8   1.47 20.8  61   \n 7     7 31.4  12.1   2.42  24.9  20.6  26.5  31.8  41.2  -1.71 20.7  NA   \n 8     8 17.3   7.17  2.87  15.7   9.85 16.3  22.2  23.4   1.68 10.1  64   \n 9     9  7.75  5.69 -4.52   6.63  5.53  8.24  8.26 16.9  -5.44  5.35 -4.94\n10    10  9.33  3.00  0.677  2.96  3.42 13.4   9.70 13.7  -5.10  4.48 NA   \n11    11  7.56  3.10  3.08   3.52  2.85  9.83 13.5  16.9  -5.44  3.28  0.8 \n12    12 19.8  11.7  18.0   17.0  10.8  27.9  13.1  26.1  -3.14 12.7  NA   \n# ℹ 5 more variables: UA &lt;dbl&gt;, US &lt;dbl&gt;, VX &lt;dbl&gt;, WN &lt;dbl&gt;, YV &lt;dbl&gt;\n\n\nCalculate the proportion of flights that were delayed (dep_delay greater than 0) but arrived on or before time (arr_delay less than or equal to 0).\n\n\nflights |&gt; \n  summarise(\n    count = sum(dep_delay &gt;0 & arr_delay &lt;=0, na.rm = T),\n    total_flights = n(),\n    proportion = count/total_flights\n  )\n\n# A tibble: 1 × 3\n  count total_flights proportion\n  &lt;int&gt;         &lt;int&gt;      &lt;dbl&gt;\n1 35442        336776      0.105\n\n\nnote: summarise is helpful for calculating summary statistics for the entire dataset/ groups\n\nUsing the airlines and flights datasets, do the following, showing the output from each step:\n6.1) Identify routes that more than one airline flies\n\n\nlibrary(dplyr)  \nflights\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\nmult_flights &lt;- flights |&gt;   \n  group_by(origin, dest) |&gt;  \n  #summarise(num_airlines = n_distinct(carrier), .group = 'drop') %&gt;% \n  filter(n_distinct(carrier) &gt; 1) |&gt;   \n  select(origin, dest, carrier) |&gt;   \ndistinct()\nmult_flights \n\n# A tibble: 343 × 3\n# Groups:   origin, dest [128]\n   origin dest  carrier\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;  \n 1 JFK    MIA   AA     \n 2 LGA    ATL   DL     \n 3 EWR    ORD   UA     \n 4 EWR    FLL   B6     \n 5 LGA    IAD   EV     \n 6 JFK    MCO   B6     \n 7 LGA    ORD   AA     \n 8 JFK    TPA   B6     \n 9 JFK    LAX   UA     \n10 EWR    SFO   UA     \n# ℹ 333 more rows\n\n\n6.2) For each such route, calculate the average arrival delay for each airline (exclude NAs). Find the names of these airlines.\n\nlibrary(dplyr)  # Load dplyr for data manipulation  \n\n# Create a summarized list of routes with any airline (if needed)  \nunique_routes &lt;- mult_flights |&gt;   \n  group_by(origin, dest) |&gt;   \n  summarise()  # This will keep only unique routes  \n\n`summarise()` has grouped output by 'origin'. You can override using the\n`.groups` argument.\n\n# Calculate average arrival delays for airlines on identified multi-airline routes using join  \navg_arrival &lt;- flights |&gt;   \n  \n  # Join the flights dataset with the unique routes on origin and dest  \n  inner_join(unique_routes,   \n             by = c(\"origin\", \"dest\")) |&gt;  # Joining on origin and dest  \n  \n  group_by(origin, dest, carrier) |&gt;   # Group by origin, destination, and carrier  \n  summarise(mean_arr_delay = mean(arr_delay, na.rm = TRUE), .groups = 'drop')  # Calculate mean arrival delay  \n\n# Display the average arrival delays  \navg_arrival\n\n# A tibble: 343 × 4\n   origin dest  carrier mean_arr_delay\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;            &lt;dbl&gt;\n 1 EWR    ATL   9E               -6.25\n 2 EWR    ATL   DL               10.0 \n 3 EWR    ATL   EV               19.5 \n 4 EWR    ATL   UA               10.5 \n 5 EWR    AUS   UA                4.28\n 6 EWR    AUS   WN              -11.2 \n 7 EWR    BDL   EV                6.78\n 8 EWR    BDL   UA               22.6 \n 9 EWR    BNA   EV               17.7 \n10 EWR    BNA   WN               -2.13\n# ℹ 333 more rows\n\n\n6.3) For each such route, identify the airline with the worst and best average arrival delay.\n\nbest_worst &lt;- avg_arrival |&gt;   \n  group_by(origin, dest) |&gt;   \n  summarise(best_airline = carrier[which.min(mean_arr_delay)],  \n            best_delay = min(mean_arr_delay),  \n            \n            worst_airline = carrier[which.max(mean_arr_delay)],  \n            worst_delay = max(mean_arr_delay),\n            \n            difference = max(mean_arr_delay) - min(mean_arr_delay)) \n\n`summarise()` has grouped output by 'origin'. You can override using the\n`.groups` argument.\n\nbest_worst\n\n# A tibble: 128 × 7\n# Groups:   origin [3]\n   origin dest  best_airline best_delay worst_airline worst_delay difference\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt;\n 1 EWR    ATL   9E               -6.25  EV                  19.5       25.8 \n 2 EWR    AUS   WN              -11.2   UA                   4.28      15.5 \n 3 EWR    BDL   EV                6.78  UA                  22.6       15.8 \n 4 EWR    BNA   WN               -2.13  EV                  17.7       19.8 \n 5 EWR    BOS   EV               -4.01  B6                   6.87      10.9 \n 6 EWR    BWI   WN                5.95  EV                  20.1       14.1 \n 7 EWR    CHS   UA              -14     EV                  16.2       30.2 \n 8 EWR    CLE   EV               -3.71  UA                   5.97       9.68\n 9 EWR    CLT   US                0.920 EV                  20.5       19.6 \n10 EWR    CVG   9E                1.40  EV                  21.2       19.8 \n# ℹ 118 more rows\n\n\n6.4) Identify the route with the greatest difference between the best and worst performing airlines\n\n best_worst |&gt;   \n  filter(difference == max(difference)) \n\n# A tibble: 2 × 7\n# Groups:   origin [2]\n  origin dest  best_airline best_delay worst_airline worst_delay difference\n  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt;\n1 EWR    STL   WN                13.6  UA                    110       96.4\n2 JFK    ATL   9E                 1.40 EV                    128      127. \n\n\nThe difference in arrival delays between these airlines could be due to weather-related factors which might affect some airlines more than others.\n\n\nQuestion 7: Identify all columns with missing entries, typos and any other inconsistencies in the dataset below\nFirst we read in the dataset into object data\n\ndata &lt;- structure(list(id = c(\"id_1\", \"id_2\", \"id_3\", \"id_4\", \"id_5\", \n\"id_6\", \"id_7\", \"id_8\", \"id_9\", \"id_10\", \"id_11\", \"id_12\", \"id_13\", \n\"id_14\", \"id_15\", \"id_16\", \"id_17\", \"id_18\", \"id_19\", \"id_20\", \n\"id_21\", \"id_22\", \"id_23\", \"id_24\", \"id_25\", \"id_26\", \"id_27\", \n\"id_28\", \"id_29\", \"id_30\", \"id_31\", \"id_32\", \"id_33\", \"id_34\", \n\"id_35\", \"id_36\", \"id_37\", \"id_38\", \"id_39\", \"id_40\", \"id_41\", \n\"id_42\", \"id_43\", \"id_44\", \"id_45\", \"id_46\", \"id_47\", \"id_48\", \n\"id_49\", \"id_50\"), age = c(50L, 34L, 70L, 33L, 22L, 61L, 69L, \n73L, 62L, 56L, 71L, 33L, 73L, 44L, 45L, 46L, 24L, 70L, 46L, 76L, \n47L, 76L, 28L, 48L, 54L, 27L, 45L, 26L, 61L, 28L, 38L, 55L, 33L, \n36L, 62L, 58L, 72L, 31L, 34L, 51L, 61L, 64L, 26L, 28L, 60L, 29L, \n42L, 46L, 79L, 72L), gender = c(\"male\", \"male\", \"male\", \"female\", \n\"female\", \"male\", \"female\", \"male\", \"male\", \"female\", \"female\", \n\"male\", \"male\", \"female\", \"male\", \"male\", \"male\", \"male\", \"female\", \n\"male\", \"male\", \"male\", \"male\", \"female\", \"femal\", \"male\", \"female\", \n\"female\", \"female\", \"female\", \"male\", \"female\", \"female\", \"female\", \n\"male\", \"male\", \"female\", \"male\", \"female\", \"female\", \"male\", \n\"female\", \"female\", \"male\", \"male\", \"female\", \"male\", \"male\", \n\"male\", \"female\"), height = c(174.4, 197.7, 174.1, 194.5, NA, \n180.4, 170.5, 157.4, 196.8, 165.1, 153, 197.4, 186, 157.1, 177.5, \n197.7, 179.3, 170.2, 182.4, NA, 165.4, 161, 168.5, 199.2, 157.7, \n154.6, 157.1, 184.5, 181, 194.6, 183.6, 186.9, 176.1, 183, 191.1, \n189.3, 199, 172, 165.6, 170.5, 150.5, 159.2, 192.1, 161.6, 162, \n153.8, 162.3, 186.6, 192.4, 174.9), weight = c(69.4, 62.3, 55.6, \n69.5, 78.6, 60.8, 72.2, 60.9, 75.1, 67.7, 82.5, 68.7, 67.8, 76.7, \n87, 61.1, 70.6, 63.3, 81.5, 59.2, 93.2, 87.3, 83.4, 80.9, 68.6, \n76.5, 93.7, 79.1, 92, 65.6, 85.4, 63.3, 79.7, 74.1, 63.3, 78.2, \n95.7, 95.1, 63.7, 66.1, 99.3, 81, 96.9, 73.3, 70.3, 83, 57.6, \n78.6, 61.9, 98.1), blood_type = c(\"O\", \"A\", \"O\", \"O\", \"B\", \"AB\", \n\"O\", \"O\", \"O\", \"AB\", \"A\", \"O\", \"O\", \"O\", \"B\", \"A\", \"B\", \"AB\", \n\"O\", \"AB\", \"A\", \"AB\", \"O\", \"B\", \"A\", \"A\", \"B\", \"AB\", \"A\", \"B\", \n\"B\", \"A\", \"O\", \"O\", \"O\", \"B\", \"O\", \"A\", \"A\", \"B\", \"A\", \"O\", \"AB\", \n\"A\", \"A\", \"O\", \"O\", \"B\", \"A\", \"O\"), disease_status = c(\"diseased\", \n\"healthy\", \"healthy\", \"healthy\", \"healthy\", \"healthy\", \"diseased\", \n\"healthy\", \"diseased\", \"Healthy\", \"diseased\", \"healthy\", \"diseased\", \n\"healthy\", \"diseased\", \"healthy\", \"healthy\", \"healthy\", \"healthy\", \n\"healthy\", \"healthy\", \"diseased\", \"healthy\", \"diseased\", \"healthy\", \n\"healthy\", \"healthy\", \"healthy\", \"diseased\", \"diseased\", \"healthy\", \n\"healthy\", \"healthy\", \"diseased\", \"diseased\", \"diseased\", \"healthy\", \n\"diseased\", \"healthy\", \"healthy\", \"healthy\", \"healthy\", \"healthy\", \n\"diseased\", \"diseased\", \"diseased\", \"healthy\", \"healthy\", \"diseased\", \n\"diseased\"), cholesterol = c(228, 223, 213, 198, 166, 151, 195, \n199, 189, 196, 221, 156, 185, 230, 234, 174, 185, 236, 235, 180, \n165, 220, 160, 153, 250, 153, 184, 242, 212, 179, 224, 233, 181, \n199, 220, 214, 214, 248, 191, 162, 203, 173, 199, 187, 248, 189, \n173, 212, 164, 247), glucose = c(96, 78, 101, 119, 103, 91, 86, \nNA, 77, 80, 115, 85, 88, 109, NA, 71, 90, 94, 91, 87, 113, 93, \n97, 118, 109, 80, 85, 119, 99, 108, 89, 108, 97, 116, 79, 84, \n75, 81, 119, NA, 106, 109, 75, 82, 84, 75, 76, 120, 119, 77), \n    smoker = c(\"yes\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"no\", \n    \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"yes\", \n    \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"no\", \n    \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"yes\", \"no\", \"yes\", \n    \"no\", \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"yes\", \n    \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"yes\"), exercise = c(\"occasional\", \n    \"regular\", \"occasional\", \"regular\", \"none\", \"occasional\", \n    \"regular\", \"none\", \"occasional\", \"none\", \"occasional\", \"none\", \n    \"none\", \"regular\", \"occasional\", \"none\", \"regular\", \"regular\", \n    \"none\", \"occasional\", \"none\", \"occasional\", \"occasional\", \n    \"occasional\", \"regular\", \"occasional\", \"regular\", \"regular\", \n    \"regular\", \"occasional\", \"occasional\", \"none\", \"none\", \"regular\", \n    \"occasional\", \"occasional\", \"none\", \"none\", \"none\", \"none\", \n    \"occasional\", \"regular\", \"regular\", \"none\", \"regular\", \"occasional\", \n    \"occasional\", \"none\", \"occasional\", \"regular\"), income = c(84820L, \n    81547L, 22588L, 72490L, 74533L, 25338L, 41469L, 57315L, 63629L, \n    88662L, 62615L, 56261L, 58499L, 82232L, 77584L, 77275L, 38468L, \n    54510L, 91326L, 78611L, 31402L, 29586L, 21441L, 58269L, 84173L, \n    88295L, 37940L, 43750L, 69750L, 92356L, 82518L, 91455L, 68866L, \n    51178L, 68275L, 27689L, 35418L, 81318L, 62405L, 86851L, 25654L, \n    47553L, 74474L, 51409L, 22607L, 55360L, 96351L, 21516L, 41927L, \n    55810L), education = c(\"master\", \"bachelor\", \"PhD\", \"master\", \n    \"bachelor\", \"highschool\", \"PhD\", \"highschool\", \"PhD\", \"PhD\", \n    \"bachelor\", \"highschool\", \"master\", \"bachelor\", \"PhD\", \"PhD\", \n    \"PhD\", \"bachelor\", \"master\", \"highschool\", \"PhD\", \"highschool\", \n    \"bachelor\", \"master\", \"highschool\", \"highschool\", \"master\", \n    \"master\", \"bachelor\", \"PhD\", \"highschool\", \"PhD\", \"master\", \n    \"master\", \"master\", \"PhD\", \"highschool\", \"master\", \"master\", \n    \"highschool\", \"bachelor\", \"highschool\", \"bachelor\", \"PhD\", \n    \"bachelor\", \"highschool\", \"master\", \"highschool\", \"bachelor\", \n    \"bachelor\"), region = c(\"North\", \"South\", \"North\", \"West\", \n    \"North\", \"West\", \"South\", \"South\", \"West\", \"South\", \"West\", \n    \"South\", \"West\", \"East\", \"North\", \"West\", \"North\", \"North\", \n    \"West\", \"North\", \"East\", \"West\", \"South\", \"North\", \"North\", \n    \"East\", \"East\", \"North\", \"North\", \"West\", \"South\", \"West\", \n    \"West\", \"East\", \"West\", \"North\", \"West\", \"North\", \"East\", \n    \"North\", \"West\", \"South\", \"South\", \"East\", \"North\", \"West\", \n    \"West\", \"East\", \"North\", \"East\"), marital_status = c(\"divorced\", \n    \"single\", \"divorced\", \"divorced\", \"divorced\", \"divorced\", \n    \"divorced\", \"married\", \"divorced\", \"married\", \"divorced\", \n    \"widowed\", \"married\", \"single\", \"widowed\", \"widowed\", \"single\", \n    \"divorced\", \"widowed\", \"widowed\", \"single\", \"married\", \"single\", \n    \"married\", \"widowed\", \"married\", \"single\", \"single\", \"widowed\", \n    \"married\", \"widowed\", \"divorced\", \"single\", \"married\", \"single\", \n    \"widowed\", \"widowed\", \"married\", \"widowed\", \"divorced\", \"married\", \n    \"married\", \"divorced\", \"single\", \"married\", \"widowed\", \"divorced\", \n    \"divorced\", \"single\", \"divorced\")), row.names = c(NA, -50L\n), class = c(\"tbl_df\", \"tbl\", \"data.frame\"))\n\ndata\n\n# A tibble: 50 × 15\n   id      age gender height weight blood_type disease_status cholesterol\n   &lt;chr&gt; &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;                &lt;dbl&gt;\n 1 id_1     50 male     174.   69.4 O          diseased               228\n 2 id_2     34 male     198.   62.3 A          healthy                223\n 3 id_3     70 male     174.   55.6 O          healthy                213\n 4 id_4     33 female   194.   69.5 O          healthy                198\n 5 id_5     22 female    NA    78.6 B          healthy                166\n 6 id_6     61 male     180.   60.8 AB         healthy                151\n 7 id_7     69 female   170.   72.2 O          diseased               195\n 8 id_8     73 male     157.   60.9 O          healthy                199\n 9 id_9     62 male     197.   75.1 O          diseased               189\n10 id_10    56 female   165.   67.7 AB         Healthy                196\n# ℹ 40 more rows\n# ℹ 7 more variables: glucose &lt;dbl&gt;, smoker &lt;chr&gt;, exercise &lt;chr&gt;,\n#   income &lt;int&gt;, education &lt;chr&gt;, region &lt;chr&gt;, marital_status &lt;chr&gt;\n\n\nIdentify all columns with missing entries, typos and any other inconsistencies in the dataset above\n\n#convert dataset into a tibble \n\nlibrary(tibble)\nlibrary(utils)\ndata &lt;- as_tibble(data)\ndata\n\n# A tibble: 50 × 15\n   id      age gender height weight blood_type disease_status cholesterol\n   &lt;chr&gt; &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;                &lt;dbl&gt;\n 1 id_1     50 male     174.   69.4 O          diseased               228\n 2 id_2     34 male     198.   62.3 A          healthy                223\n 3 id_3     70 male     174.   55.6 O          healthy                213\n 4 id_4     33 female   194.   69.5 O          healthy                198\n 5 id_5     22 female    NA    78.6 B          healthy                166\n 6 id_6     61 male     180.   60.8 AB         healthy                151\n 7 id_7     69 female   170.   72.2 O          diseased               195\n 8 id_8     73 male     157.   60.9 O          healthy                199\n 9 id_9     62 male     197.   75.1 O          diseased               189\n10 id_10    56 female   165.   67.7 AB         Healthy                196\n# ℹ 40 more rows\n# ℹ 7 more variables: glucose &lt;dbl&gt;, smoker &lt;chr&gt;, exercise &lt;chr&gt;,\n#   income &lt;int&gt;, education &lt;chr&gt;, region &lt;chr&gt;, marital_status &lt;chr&gt;\n\n#unique values in each column \nUtilsDataRSV::view_cols(data)\n\n[1] \"id\"\n [1] \"id_14\" \"id_22\" \"id_24\" \"id_43\" \"id_19\" \"id_2\"  \"id_16\" \"id_30\" \"id_45\"\n[10] \"id_42\" \"id_3\"  \"id_28\" \"id_25\" \"id_41\" \"id_36\" \"id_4\"  \"id_33\" \"id_6\" \n[19] \"id_18\" \"id_38\"\n[1] \"30 unique entries not displayed\"\n[1] \"_____________________\"\n[1] \"age\"\n[1] 69 47 45 76 27\n[1] \"_____________________\"\n[1] \"gender\"\n[1] \"female\" \"femal\"  \"male\"  \n[1] \"_____________________\"\n[1] \"height\"\n[1] 170.5 194.6 177.5 192.4    NA\n[1] \"_____________________\"\n[1] \"weight\"\n[1] 65.6 98.1 60.8 63.3 70.6\n[1] \"_____________________\"\n[1] \"blood_type\"\n[1] \"O\"  \"A\"  \"AB\" \"B\" \n[1] \"_____________________\"\n[1] \"disease_status\"\n[1] \"healthy\"  \"Healthy\"  \"diseased\"\n[1] \"_____________________\"\n[1] \"cholesterol\"\n[1] 220 213 214 185 248\n[1] \"_____________________\"\n[1] \"glucose\"\n[1]  77 116  71  90  NA\n[1] \"_____________________\"\n[1] \"smoker\"\n[1] \"no\"  \"yes\"\n[1] \"_____________________\"\n[1] \"exercise\"\n[1] \"regular\"    \"none\"       \"occasional\"\n[1] \"_____________________\"\n[1] \"income\"\n[1] 74533 51178 63629 58269 62615\n[1] \"_____________________\"\n[1] \"education\"\n[1] \"bachelor\"   \"PhD\"        \"highschool\" \"master\"    \n[1] \"_____________________\"\n[1] \"region\"\n[1] \"East\"  \"South\" \"North\" \"West\" \n[1] \"_____________________\"\n[1] \"marital_status\"\n[1] \"single\"   \"divorced\" \"widowed\"  \"married\" \n[1] \"_____________________\"\n\n\nWarning: Not all unique entries displayed for these non-numeric cols: id\n\n\n\nMissing Values: found under height and glucose variables i.e. NA values\nInconsistencies: found under the “disease_status” for level healthy i.e. “healthy” “Healthy”\nTypos: found under gender i.e.\n\"femal\"  \"female\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Prac Day 4</span>"
    ]
  },
  {
    "objectID": "prac_2.html",
    "href": "prac_2.html",
    "title": "Prac 2",
    "section": "",
    "text": "LOWESS PRACTICAL\nThe goal of Lowess smoothing is to create a smooth curve that captures the underlying trend of the data while accounting for local structures and variation.\nThis is especially useful in datasets like noisy sine waves where the true relationship is obscured by noise",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Prac 2</span>"
    ]
  },
  {
    "objectID": "prac_2.html#lowess-practical",
    "href": "prac_2.html#lowess-practical",
    "title": "Prac 2",
    "section": "",
    "text": "Steps to Implement Lowess Smoothing\n\nUnderstanding the data\n\nGoal: Gather the observed data pairs as (x1​,y1​),…,(xn​,yn​), where x represents the independent variable and y is the dependent variable. We will generate sequences for x and create the corresponding y values using a sine function with added noise.\n\n\nChoosing the Span\n\nGoal: Select a smoothing parameter (span) f where 0&lt;f&lt;1. This span determines how many neighboring points will influence the smoothed estimate for each xi\nCalculation k=⌈f⋅n⌉, where k is the number of closest neighbours used for point xi and n is the number of observations.\n\n\nComputing Weights - Goal: For each point xi , calculate the weight for each of its neighbors using the tricube kernel:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Prac 2</span>"
    ]
  },
  {
    "objectID": "prac_2.html#practical-question",
    "href": "prac_2.html#practical-question",
    "title": "Prac 2",
    "section": "PRACTICAL QUESTION",
    "text": "PRACTICAL QUESTION\n\n#set seed to 1\nset.seed(1)\n\n#Generate simulated data \nn &lt;- 100\nx &lt;- seq(1:n)\ne &lt;- rnorm(n, mean = 0 , sd = 0.2)\ny &lt;- sin(x/10) + e\n\n#Implement the LOWESS Algorithm\ncustomLowess &lt;- function(x, y, f)\n{\n  nobs &lt;-  length(x)\n  smoothed_y &lt;- numeric(nobs) #Initialize storage for smoothed y values \n  \n  #determine k = number of closest neighbors based on the bandwidth (f)\n  k = ceiling(f * nobs)\n  \n  #compute weights \n  #Calculate dmax \n  for (i in 1:n){\n    #caclulate distance from current point \n    dist = abs(x - x[i]) #distance vector\n    sort_dist_indices &lt;-  order(dist) #give indices of sorted distance in ascending order \n    \n    #find k closest neighbours \n    near_indices &lt;- sort_dist_indices[1:k] #By taking the first k of these sorted indices, we get the indices of the nearest points\n    \n    #find corresponding co-ord for nearest neighbour points \n    x_neighbours &lt;-  x[near_indices]#closest points are at those indices\n    y_neighbours &lt;-  y[near_indices]\n    nearest_dist &lt;- dist[near_indices]\n    \n    #Compute weights\n    d_max &lt;- max(nearest_dist)\n    wj &lt;- (1-(abs(nearest_dist)/d_max)^3)^3\n    wj[abs(nearest_dist) &gt;= d_max] &lt;- 0 #condition is TRUE, the corresponding weight in the weights vector is set to 0\n    \n    X &lt;- cbind(1, x_neighbours)\n    W &lt;- diag(wj) # Create diagonal weight matrix  \n    \n    #regression estimates\n    beta_hat &lt;- solve(t(X) %*% W %*% X) %*% t(X) %*% W %*% y_neighbours\n    \n    smoothed_y[i] &lt;- beta_hat[1] + beta_hat[2] %*% x[i]\n    \n  }\n  return(list(x = x, smoothed_y =  smoothed_y))\n}\n\nCalling the above customLowess function and compare it with the built-in lowess() function with the same f value. Note: iter argument to 0\nBelow displays the plotted results for the customLowess function for f = 0.5\n\nlibrary(graphics)\nlibrary(ggplot2)\nresult &lt;- customLowess(x, y, f = 0.5)\n\n# Plot the original data  \nplot(y ~ x, main = \"LOWESS Smoothing\", xlab = \"X-axis\", ylab = \"Y-axis\", pch = 19) \n\n#add lowess line \nlines(result$x, result$smoothed_y, col = 'blue', lwd = 2)\n\n\n\n\n\n\n\n\nBelow displays the plotted results or the build-in Lowess function for f = 0.5\n\n# Plot the original data  \nplot(y ~ x, main = \"LOWESS Smoothing\", xlab = \"X-axis\", ylab = \"Y-axis\", pch = 19) \n\nlines(lowess(x, y, f=0.5, iter = 0),col = 'red', lwd = 2)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Prac 2</span>"
    ]
  }
]